{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival prediction using Spark\n",
    "\n",
    "I use the following classification models:\n",
    " * Logistic Regression \n",
    " * SVM *(there's a chance the data is simple enough to be linearly separable)*\n",
    " * Naive Bayes *(the simplest model one can think of but sometimes it shows unexpectedly good results, so I think it's interesting to see what it can do with Titanic data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with settings, then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-oracle'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = '/home/asti/spark-2.2.0-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionModel, LinearRegressionWithSGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf().setMaster(\"local\")\n",
    "        .setAppName(\"Homework\")\n",
    "        .set(\"spark.executor.memory\", \"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns will go with *'string'* type just fine, but some should be casted\n",
    "\n",
    "* Should be **integers**: ticket class (**Pclass**), survival indicator (**Survived**), number of family members onboard (**SibSp** and **Parch**)\n",
    "* Should be **boolean**: gender (**Sex**)\n",
    "* Will go with **float/double**: **Age**, **Fare**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably an absolute value of one's age isn't as important as the age group this person belongs to. But before dividing age values into groups let's preprocess it first.\n",
    "\n",
    "Some values are missed in *Age* column. I'm going to cast the column to double type and then fill all the gaps with the value of mean age in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_age(str_age):\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "change_age = udf(parse_age, types.StringType())\n",
    "df = df.withColumn('Age', change_age(df['Age']))\n",
    "\n",
    "df = df.withColumn(\"Age\",df[\"Age\"].cast(types.DoubleType()).alias(\"Age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_age = df.approxQuantile(\"Age\", [0.5], 0.1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_and_divide_age(age):\n",
    "    fixed = age if age else mean_age\n",
    "    # newborns and babies\n",
    "    if fixed < 3.0:\n",
    "        return 1\n",
    "    # little children\n",
    "    if 3.0 <= fixed < 10.0:\n",
    "        return 2\n",
    "    # teenagers\n",
    "    if 10.0 <= fixed < 17.0:\n",
    "        return 3\n",
    "    if 17.0 <= fixed < 22.0:\n",
    "        return 4\n",
    "    if 22.0 <= fixed < 35.0:\n",
    "        return 5\n",
    "    if 35.0 <= fixed < 50.0:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "    \n",
    "fix_age = udf(fix_and_divide_age, types.IntegerType())\n",
    "df = df.withColumn('Age', fix_age(df['Age']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get titles from names\n",
    "\n",
    "For this dataset it's common to extract titles from given names (e.g. Mr., Mrs.).\n",
    "\n",
    "There are not enough data for titles like, 'Don', 'Dr', and Dutch 'Jonkheer' to have considerable impact on predictions, so I'm going to replace these with corresponding common English titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "            'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme',\n",
    "            'Countess', 'Don', 'Jonkheer']\n",
    "\n",
    "def replace_title(name, sex):\n",
    "    def find_title(name):\n",
    "        for title in title_list:\n",
    "            if title in name:\n",
    "                return title\n",
    "        return None\n",
    "    \n",
    "    title = find_title(name)\n",
    "    \n",
    "    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "        return 'Mr'\n",
    "    elif title in ['Countess', 'Mme']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title =='Dr':\n",
    "        if sex=='male':\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    else:\n",
    "        return title\n",
    "\n",
    "get_title = udf(replace_title, types.StringType())\n",
    "df = df.withColumn('Title', get_title(df['Name'], df['Sex']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missed data and get info about decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix 'Embarked' column, fill gaps with 'Unknown' value\n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Unk'\n",
    "        \n",
    "get_embarked = udf(Embarked_transform, types.StringType())\n",
    "df = df.withColumn('Embarked', get_embarked(df['Embarked']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the cabins were marked according to the deck they were on. Maybe where people were located on the ship had some impact on their survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cabin_transform(cabin):\n",
    "    if cabin:\n",
    "        return cabin[0]\n",
    "    else:\n",
    "        return 'Unk'\n",
    "    \n",
    "get_deck = udf(cabin_transform, types.StringType())\n",
    "df = df.withColumn('Deck', get_deck(df['Cabin']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode 'Embarked', 'Deck', 'Age' and 'Title' columns\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# --------\n",
    "embarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "e_model = embarkedIndexer.fit(df)\n",
    "e_indexed = e_model.transform(df)\n",
    "\n",
    "e_encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "df = e_encoder.transform(e_indexed)\n",
    "# --------\n",
    "# --------\n",
    "deckIndexer = StringIndexer(inputCol='Deck', outputCol='DeckIndex')\n",
    "d_model = deckIndexer.fit(df)\n",
    "d_indexed = d_model.transform(df)\n",
    "\n",
    "d_encoder = OneHotEncoder(inputCol='DeckIndex', outputCol='DeckVec')\n",
    "df = d_encoder.transform(d_indexed)\n",
    "# --------\n",
    "# --------\n",
    "age_encoder = OneHotEncoder(inputCol='Age', outputCol='AgeVec')\n",
    "df = age_encoder.transform(df)\n",
    "# --------\n",
    "# --------\n",
    "titleIndexer = StringIndexer(inputCol='Title', outputCol='TitleIndex')\n",
    "t_model = titleIndexer.fit(df)\n",
    "t_indexed = t_model.transform(df)\n",
    "\n",
    "t_encoder = OneHotEncoder(inputCol='TitleIndex', outputCol='TitleVec')\n",
    "df = t_encoder.transform(t_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally getting data in the needed form for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(row):\n",
    "    return LabeledPoint(\n",
    "        int(row.Survived),\n",
    "        [\n",
    "            int(row.Pclass),\n",
    "            row.Sex == 'male',        \n",
    "            int(row.SibSp),\n",
    "            int(row.Parch),\n",
    "            float(row.Fare),\n",
    "        ]\n",
    "        + list(row.TitleVec.toArray())\n",
    "        + list(row.AgeVec.toArray())\n",
    "        + list(row.DeckVec.toArray())\n",
    "        + list(row.EmbarkedVec.toArray())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.rdd.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "def print_metrics(cls, test_data):\n",
    "    def f1_score(metrics):\n",
    "        return 2 * (metrics.recall(1.0) * metrics.precision(1.0)) / (metrics.recall(1.0) + metrics.precision(1.0))\n",
    "    \n",
    "    scoreAndLabels = test_data.map(lambda x : (float(cls.predict(x.features)), x.label))\n",
    "    metrics = MulticlassMetrics(scoreAndLabels)\n",
    "    \n",
    "    print(\"Accuracy of the model: {}\" .format(metrics.accuracy))\n",
    "    print(\"          with recall: {}\" .format(metrics.recall(1.0)))\n",
    "    print(\"        and precision: {}\" .format(metrics.precision(1.0)))\n",
    "    \n",
    "    print(\"\\nF1 score of the model: {}\" .format(f1_score(metrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "lg = LogisticRegressionWithLBFGS.train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.818532818533\n",
      "          with recall: 0.704761904762\n",
      "        and precision: 0.822222222222\n",
      "\n",
      "F1 score of the model: 0.758974358974\n"
     ]
    }
   ],
   "source": [
    "print_metrics(lg, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "*with SGD update*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "\n",
    "svm = SVMWithSGD.train(train, iterations=1000, step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.698841698842\n",
      "          with recall: 0.304761904762\n",
      "        and precision: 0.864864864865\n",
      "\n",
      "F1 score of the model: 0.450704225352\n"
     ]
    }
   ],
   "source": [
    "print_metrics(svm, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.725868725869\n",
      "          with recall: 0.485714285714\n",
      "        and precision: 0.75\n",
      "\n",
      "F1 score of the model: 0.589595375723\n"
     ]
    }
   ],
   "source": [
    "print_metrics(nb, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
